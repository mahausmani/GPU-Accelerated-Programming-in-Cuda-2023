{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9Pt86BbvQZ0FS4RRe+I4Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mu06905/GPU-Accelerated-Programming-in-Cuda-2023/blob/main/Week6/DotProductReduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation:\n",
        "We have two arrays a and b. tid (global thread index) would range from [0, 33972].\n",
        "We have 256 threads per bloclks and 32 blocks.\n",
        "In the first loop, each thread of each block multiplies an element of a and b and adds them together with an offset of GridDim x blockDim, which is (32*256 = 8198). So for the first block we have:\n",
        "\n",
        "First iteration: tid = 0:\n",
        "\n",
        "temp + =a[0] x b[0] (block 0 , thread 0)\n",
        "\n",
        "temp += a[1] x b[1] (block 0, thread 1)\n",
        "\n",
        "...\n",
        "\n",
        "temp += a[256] x b[256] (block 0, thread 256)\n",
        "\n",
        "Second Iteration: tid = 8196:\n",
        "\n",
        "\n",
        "\n",
        "then tid += 8196, which gives:\n",
        "\n",
        "temp += a[8196] x b[8196] (block 0, thread 0)\n",
        "\n",
        "temp += a[8197] x b[8197] (block 0, thread 1)\n",
        "\n",
        "...\n",
        "\n",
        "Note that GridDim x BlockDim != N\n",
        "\n",
        "Now for each block we have a cache 32 elements each which conataining partial sum of the array. cache1 contains sum of \n",
        "[a[0] * b[0], a[1] x b[1] ... ,a[256] x b[256], a[8196] x b [8196] , a[8197] x b[8197] ...]\n",
        "\n",
        " cache2 contains sum of \n",
        "[a[257] * b[257], a[258] x b[258] ... ,a[512] x b[512], a[8708] x b [8708] , a[8709] x b[8709] ...]\n",
        "\n",
        "So we have 32 cahes, we use reduction sum to sum over these cache arrays and store them in the first element of the cache of each block.\n",
        "\n",
        "which is then stored inside another array and the final sum is calculated on cpu\n",
        "\n"
      ],
      "metadata": {
        "id": "HrXYPnz747RU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XUlrZfn40yz"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu \n",
        "#include <stdio.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "const int N = 33*1024;\n",
        "const int blocksPerGrid = 32;\n",
        "const int threadsPerBlock = 256; //multiple of 2 for reduction code\n",
        "const int size = N * sizeof(int);\n",
        "\n",
        "__global__ void dot(int* a, int*b, int*c, int N){\n",
        "    __shared__ int cache[threadsPerBlock];\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    int cache_index = threadIdx.x;\n",
        "    int temp = 0;\n",
        "    while(tid<N){\n",
        "        temp += a[tid] * b[tid];\n",
        "        tid += blocksPerGrid * threadsPerBlock;\n",
        "    }\n",
        "    cache[cache_index] = temp;\n",
        "    __syncthreads();\n",
        "\n",
        "    int i = blockDim.x / 2;\n",
        "    while(i != 0){\n",
        "        if (cache_index < i){\n",
        "            cache[cache_index] += cache[cache_index + i];\n",
        "        }\n",
        "        __syncthreads();\n",
        "        i/=2;\n",
        "    }\n",
        "    if(cache_index == 0)\n",
        "    c[blockIdx.x] = cache[0];\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    int* h_a = (int*)malloc(size);\n",
        "    int* h_b = (int*)malloc(size);\n",
        "    int* h_c = (int*)malloc(size);\n",
        "\n",
        "    int* d_a;\n",
        "    int *d_b;\n",
        "    int* d_c;\n",
        "\n",
        "    cudaMalloc((void**)&d_a, size);\n",
        "    cudaMalloc((void**)&d_b, size);\n",
        "    cudaMalloc((void**)&d_c, size);\n",
        "\n",
        "    for(int i = 0; i<N; i++){\n",
        "        h_a[i] = i * 2;\n",
        "        h_b[i] = i;\n",
        "        h_c[i] = 0;\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_c, h_c, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dot<<<blocksPerGrid, threadsPerBlock>>>(d_a,d_b,d_c,N);\n",
        "\n",
        "    cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    int sum = 0;\n",
        "    int j = 0;\n",
        "    for(int i = 0; i<threadsPerBlock; i++){\n",
        "        sum += h_c[i];\n",
        "        j++;\n",
        "    }\n",
        "\n",
        "    printf(\"%d\\n\", sum);\n",
        "    printf(\"%d,%d\",N*(N+1)/2,j);\n",
        "\n",
        "    cudaFree(&d_a);\n",
        "    cudaFree(&d_b);\n",
        "    cudaFree(&d_c);\n",
        "\n",
        "    free(h_a);\n",
        "    free(h_b);\n",
        "    free(h_c);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qyev4Thf9Xv2",
        "outputId": "85471a26-4f6d-4ae5-f7ba-d56f571ac9f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1005595648\n",
            "570966528,256\n"
          ]
        }
      ]
    }
  ]
}